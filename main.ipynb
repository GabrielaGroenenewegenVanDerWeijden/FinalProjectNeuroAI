{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project NeuroAI \n",
    "## Team Number 1: Femke, Tikva, Gabriela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all important packages\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1\n",
      "MPS available: True\n",
      "MPS built: True\n"
     ]
    }
   ],
   "source": [
    "# print(\"PyTorch version:\", torch.__version__)\n",
    "# print(\"MPS available:\", torch.backends.mps.is_available())\n",
    "# print(\"MPS built:\", torch.backends.mps.is_built())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/test/dogs/dog_147.jpg\n",
      "data/test/dogs/dog_219.jpg\n",
      "data/test/dogs/dog_191.jpg\n",
      "data/test/dogs/dog_344.jpg\n",
      "data/test/dogs/dog_150.jpg\n",
      "data/test/dogs/dog_227.jpg\n",
      "data/test/dogs/dog_421.jpg\n",
      "data/test/dogs/dog_380.jpg\n",
      "data/test/dogs/dog_155.jpg\n",
      "data/test/dogs/dog_141.jpg\n",
      "data/test/dogs/dog_196.jpg\n",
      "data/test/dogs/dog_551.jpg\n",
      "data/test/dogs/dog_237.jpg\n",
      "data/test/dogs/dog_236.jpg\n",
      "data/test/dogs/dog_197.jpg\n",
      "data/test/dogs/dog_168.jpg\n",
      "data/test/dogs/dog_28.jpg\n",
      "data/test/dogs/dog_354.jpg\n",
      "data/test/dogs/dog_142.jpg\n",
      "data/test/dogs/dog_181.jpg\n",
      "data/test/dogs/dog_194.jpg\n",
      "data/test/dogs/dog_369.jpg\n",
      "data/test/dogs/dog_355.jpg\n",
      "data/test/dogs/dog_124.jpg\n",
      "data/test/dogs/dog_130.jpg\n",
      "data/test/dogs/dog_534.jpg\n",
      "data/test/dogs/dog_520.jpg\n",
      "data/test/dogs/dog_521.jpg\n",
      "data/test/dogs/dog_482.jpg\n",
      "data/test/dogs/dog_59.jpg\n",
      "data/test/dogs/dog_327.jpg\n",
      "data/test/dogs/dog_443.jpg\n",
      "data/test/dogs/dog_536.jpg\n",
      "data/test/dogs/dog_244.jpg\n",
      "data/test/dogs/dog_522.jpg\n",
      "data/test/dogs/dog_442.jpg\n",
      "data/test/dogs/dog_89.jpg\n",
      "data/test/dogs/dog_240.jpg\n",
      "data/test/dogs/dog_283.jpg\n",
      "data/test/dogs/dog_123.jpg\n",
      "data/test/dogs/dog_75.jpg\n",
      "data/test/dogs/dog_519.jpg\n",
      "data/test/dogs/dog_518.jpg\n",
      "data/test/dogs/dog_461.jpg\n",
      "data/test/dogs/dog_313.jpg\n",
      "data/test/dogs/dog_528.jpg\n",
      "data/test/dogs/dog_44.jpg\n",
      "data/test/dogs/dog_476.jpg\n",
      "data/test/dogs/dog_462.jpg\n",
      "data/test/dogs/dog_258.jpg\n",
      "data/test/dogs/dog_517.jpg\n",
      "data/test/dogs/dog_43.jpg\n",
      "data/test/dogs/dog_472.jpg\n",
      "data/test/dogs/dog_464.jpg\n",
      "data/test/dogs/dog_302.jpg\n",
      "data/test/dogs/dog_68.jpg\n",
      "data/test/dogs/dog_114.jpg\n",
      "data/test/dogs/dog_303.jpg\n",
      "data/test/dogs/dog_364.jpg\n",
      "data/test/dogs/dog_563.jpg\n",
      "data/test/dogs/dog_211.jpg\n",
      "data/test/dogs/dog_173.jpg\n",
      "data/test/dogs/dog_415.jpg\n",
      "data/test/dogs/dog_398.jpg\n",
      "data/test/dogs/dog_159.jpg\n",
      "data/test/dogs/dog_213.jpg\n",
      "data/test/dogs/dog_377.jpg\n",
      "data/test/dogs/dog_177.jpg\n",
      "data/test/dogs/dog_229.jpg\n",
      "data/test/dogs/dog_360.jpg\n",
      "data/test/cats/cat_190.jpg\n",
      "data/test/cats/cat_147.jpg\n",
      "data/test/cats/cat_542.jpg\n",
      "data/test/cats/cat_595.jpg\n",
      "data/test/cats/cat_422.jpg\n",
      "data/test/cats/cat_583.jpg\n",
      "data/test/cats/cat_384.jpg\n",
      "data/test/cats/cat_586.jpg\n",
      "data/test/cats/cat_545.jpg\n",
      "data/test/cats/cat_223.jpg\n",
      "data/test/cats/cat_551.jpg\n",
      "data/test/cats/cat_587.jpg\n",
      "data/test/cats/cat_140.jpg\n",
      "data/test/cats/cat_342.jpg\n",
      "data/test/cats/cat_430.jpg\n",
      "data/test/cats/cat_418.jpg\n",
      "data/test/cats/cat_395.jpg\n",
      "data/test/cats/cat_156.jpg\n",
      "data/test/cats/cat_585.jpg\n",
      "data/test/cats/cat_234.jpg\n",
      "data/test/cats/cat_355.jpg\n",
      "data/test/cats/cat_433.jpg\n",
      "data/test/cats/cat_341.jpg\n",
      "data/test/cats/cat_332.jpg\n",
      "data/test/cats/cat_468.jpg\n",
      "data/test/cats/cat_124.jpg\n",
      "data/test/cats/cat_118.jpg\n",
      "data/test/cats/cat_520.jpg\n",
      "data/test/cats/cat_290.jpg\n",
      "data/test/cats/cat_119.jpg\n",
      "data/test/cats/cat_88.jpg\n",
      "data/test/cats/cat_496.jpg\n",
      "data/test/cats/cat_523.jpg\n",
      "data/test/cats/cat_251.jpg\n",
      "data/test/cats/cat_279.jpg\n",
      "data/test/cats/cat_244.jpg\n",
      "data/test/cats/cat_60.jpg\n",
      "data/test/cats/cat_446.jpg\n",
      "data/test/cats/cat_268.jpg\n",
      "data/test/cats/cat_255.jpg\n",
      "data/test/cats/cat_109.jpg\n",
      "data/test/cats/cat_525.jpg\n",
      "data/test/cats/cat_281.jpg\n",
      "data/test/cats/cat_94.jpg\n",
      "data/test/cats/cat_313.jpg\n",
      "data/test/cats/cat_1.jpg\n",
      "data/test/cats/cat_528.jpg\n",
      "data/test/cats/cat_306.jpg\n",
      "data/test/cats/cat_56.jpg\n",
      "data/test/cats/cat_106.jpg\n",
      "data/test/cats/cat_113.jpg\n",
      "data/test/cats/cat_96.jpg\n",
      "data/test/cats/cat_473.jpg\n",
      "data/test/cats/cat_116.jpg\n",
      "data/test/cats/cat_464.jpg\n",
      "data/test/cats/cat_114.jpg\n",
      "data/test/cats/cat_538.jpg\n",
      "data/test/cats/cat_504.jpg\n",
      "data/test/cats/cat_5.jpg\n",
      "data/test/cats/cat_358.jpg\n",
      "data/test/cats/cat_417.jpg\n",
      "data/test/cats/cat_371.jpg\n",
      "data/test/cats/cat_575.jpg\n",
      "data/test/cats/cat_574.jpg\n",
      "data/test/cats/cat_158.jpg\n",
      "data/test/cats/cat_564.jpg\n",
      "data/test/cats/cat_203.jpg\n",
      "data/test/cats/cat_375.jpg\n",
      "data/test/cats/cat_162.jpg\n",
      "data/test/cats/cat_18.jpg\n",
      "data/train/dogs/dog_423.jpg\n",
      "data/train/dogs/dog_345.jpg\n",
      "data/train/dogs/dog_351.jpg\n",
      "data/train/dogs/dog_437.jpg\n",
      "data/train/dogs/dog_13.jpg\n",
      "data/train/dogs/dog_392.jpg\n",
      "data/train/dogs/dog_231.jpg\n",
      "data/train/dogs/dog_543.jpg\n",
      "data/train/dogs/dog_580.jpg\n",
      "data/train/dogs/dog_224.jpg\n",
      "data/train/dogs/dog_556.jpg\n",
      "data/train/dogs/dog_230.jpg\n",
      "data/train/dogs/dog_218.jpg\n",
      "data/train/dogs/dog_152.jpg\n",
      "data/train/dogs/dog_393.jpg\n",
      "data/train/dogs/dog_350.jpg\n",
      "data/train/dogs/dog_436.jpg\n",
      "data/train/dogs/dog_378.jpg\n",
      "data/train/dogs/dog_408.jpg\n",
      "data/train/dogs/dog_391.jpg\n",
      "data/train/dogs/dog_10.jpg\n",
      "data/train/dogs/dog_178.jpg\n",
      "data/train/dogs/dog_144.jpg\n",
      "data/train/dogs/dog_193.jpg\n",
      "data/train/dogs/dog_540.jpg\n",
      "data/train/dogs/dog_226.jpg\n",
      "data/train/dogs/dog_232.jpg\n",
      "data/train/dogs/dog_554.jpg\n",
      "data/train/dogs/dog_583.jpg\n",
      "data/train/dogs/dog_596.jpg\n",
      "data/train/dogs/dog_582.jpg\n",
      "data/train/dogs/dog_541.jpg\n",
      "data/train/dogs/dog_186.jpg\n",
      "data/train/dogs/dog_192.jpg\n",
      "data/train/dogs/dog_145.jpg\n",
      "data/train/dogs/dog_151.jpg\n",
      "data/train/dogs/dog_11.jpg\n",
      "data/train/dogs/dog_39.jpg\n",
      "data/train/dogs/dog_347.jpg\n",
      "data/train/dogs/dog_435.jpg\n",
      "data/train/dogs/dog_409.jpg\n",
      "data/train/dogs/dog_343.jpg\n",
      "data/train/dogs/dog_419.jpg\n",
      "data/train/dogs/dog_9.jpg\n",
      "data/train/dogs/dog_15.jpg\n",
      "data/train/dogs/dog_29.jpg\n",
      "data/train/dogs/dog_182.jpg\n",
      "data/train/dogs/dog_545.jpg\n",
      "data/train/dogs/dog_579.jpg\n",
      "data/train/dogs/dog_550.jpg\n",
      "data/train/dogs/dog_140.jpg\n",
      "data/train/dogs/dog_154.jpg\n",
      "data/train/dogs/dog_381.jpg\n",
      "data/train/dogs/dog_418.jpg\n",
      "data/train/dogs/dog_8.jpg\n",
      "data/train/dogs/dog_342.jpg\n",
      "data/train/dogs/dog_432.jpg\n",
      "data/train/dogs/dog_368.jpg\n",
      "data/train/dogs/dog_156.jpg\n",
      "data/train/dogs/dog_195.jpg\n",
      "data/train/dogs/dog_234.jpg\n",
      "data/train/dogs/dog_552.jpg\n",
      "data/train/dogs/dog_546.jpg\n",
      "data/train/dogs/dog_220.jpg\n",
      "data/train/dogs/dog_208.jpg\n",
      "data/train/dogs/dog_591.jpg\n",
      "data/train/dogs/dog_209.jpg\n",
      "data/train/dogs/dog_221.jpg\n",
      "data/train/dogs/dog_553.jpg\n",
      "data/train/dogs/dog_433.jpg\n",
      "data/train/dogs/dog_341.jpg\n",
      "data/train/dogs/dog_427.jpg\n",
      "data/train/dogs/dog_468.jpg\n",
      "data/train/dogs/dog_440.jpg\n",
      "data/train/dogs/dog_332.jpg\n",
      "data/train/dogs/dog_70.jpg\n",
      "data/train/dogs/dog_483.jpg\n",
      "data/train/dogs/dog_291.jpg\n",
      "data/train/dogs/dog_290.jpg\n",
      "data/train/dogs/dog_247.jpg\n",
      "data/train/dogs/dog_253.jpg\n",
      "data/train/dogs/dog_535.jpg\n",
      "data/train/dogs/dog_509.jpg\n",
      "data/train/dogs/dog_125.jpg\n",
      "data/train/dogs/dog_496.jpg\n",
      "data/train/dogs/dog_333.jpg\n",
      "data/train/dogs/dog_441.jpg\n",
      "data/train/dogs/dog_98.jpg\n",
      "data/train/dogs/dog_331.jpg\n",
      "data/train/dogs/dog_325.jpg\n",
      "data/train/dogs/dog_67.jpg\n",
      "data/train/dogs/dog_494.jpg\n",
      "data/train/dogs/dog_480.jpg\n",
      "data/train/dogs/dog_133.jpg\n",
      "data/train/dogs/dog_523.jpg\n",
      "data/train/dogs/dog_251.jpg\n",
      "data/train/dogs/dog_286.jpg\n",
      "data/train/dogs/dog_292.jpg\n",
      "data/train/dogs/dog_293.jpg\n",
      "data/train/dogs/dog_287.jpg\n",
      "data/train/dogs/dog_72.jpg\n",
      "data/train/dogs/dog_324.jpg\n",
      "data/train/dogs/dog_318.jpg\n",
      "data/train/dogs/dog_99.jpg\n",
      "data/train/dogs/dog_452.jpg\n",
      "data/train/dogs/dog_334.jpg\n",
      "data/train/dogs/dog_320.jpg\n",
      "data/train/dogs/dog_446.jpg\n",
      "data/train/dogs/dog_62.jpg\n",
      "data/train/dogs/dog_76.jpg\n",
      "data/train/dogs/dog_485.jpg\n",
      "data/train/dogs/dog_136.jpg\n",
      "data/train/dogs/dog_122.jpg\n",
      "data/train/dogs/dog_532.jpg\n",
      "data/train/dogs/dog_268.jpg\n",
      "data/train/dogs/dog_296.jpg\n",
      "data/train/dogs/dog_269.jpg\n",
      "data/train/dogs/dog_533.jpg\n",
      "data/train/dogs/dog_241.jpg\n",
      "data/train/dogs/dog_77.jpg\n",
      "data/train/dogs/dog_484.jpg\n",
      "data/train/dogs/dog_490.jpg\n",
      "data/train/dogs/dog_321.jpg\n",
      "data/train/dogs/dog_447.jpg\n",
      "data/train/dogs/dog_335.jpg\n",
      "data/train/dogs/dog_445.jpg\n",
      "data/train/dogs/dog_323.jpg\n",
      "data/train/dogs/dog_337.jpg\n",
      "data/train/dogs/dog_451.jpg\n",
      "data/train/dogs/dog_479.jpg\n",
      "data/train/dogs/dog_492.jpg\n",
      "data/train/dogs/dog_109.jpg\n",
      "data/train/dogs/dog_243.jpg\n",
      "data/train/dogs/dog_525.jpg\n",
      "data/train/dogs/dog_294.jpg\n",
      "data/train/dogs/dog_281.jpg\n",
      "data/train/dogs/dog_295.jpg\n",
      "data/train/dogs/dog_256.jpg\n",
      "data/train/dogs/dog_120.jpg\n",
      "data/train/dogs/dog_487.jpg\n",
      "data/train/dogs/dog_74.jpg\n",
      "data/train/dogs/dog_478.jpg\n",
      "data/train/dogs/dog_450.jpg\n",
      "data/train/dogs/dog_444.jpg\n",
      "data/train/dogs/dog_322.jpg\n",
      "data/train/dogs/dog_92.jpg\n",
      "data/train/dogs/dog_475.jpg\n",
      "data/train/dogs/dog_86.jpg\n",
      "data/train/dogs/dog_51.jpg\n",
      "data/train/dogs/dog_45.jpg\n",
      "data/train/dogs/dog_139.jpg\n",
      "data/train/dogs/dog_529.jpg\n",
      "data/train/dogs/dog_273.jpg\n",
      "data/train/dogs/dog_501.jpg\n",
      "data/train/dogs/dog_267.jpg\n",
      "data/train/dogs/dog_298.jpg\n",
      "data/train/dogs/dog_299.jpg\n",
      "data/train/dogs/dog_266.jpg\n",
      "data/train/dogs/dog_272.jpg\n",
      "data/train/dogs/dog_514.jpg\n",
      "data/train/dogs/dog_110.jpg\n",
      "data/train/dogs/dog_138.jpg\n",
      "data/train/dogs/dog_50.jpg\n",
      "data/train/dogs/dog_78.jpg\n",
      "data/train/dogs/dog_474.jpg\n",
      "data/train/dogs/dog_93.jpg\n",
      "data/train/dogs/dog_306.jpg\n",
      "data/train/dogs/dog_448.jpg\n",
      "data/train/dogs/dog_338.jpg\n",
      "data/train/dogs/dog_85.jpg\n",
      "data/train/dogs/dog_489.jpg\n",
      "data/train/dogs/dog_106.jpg\n",
      "data/train/dogs/dog_502.jpg\n",
      "data/train/dogs/dog_270.jpg\n",
      "data/train/dogs/dog_259.jpg\n",
      "data/train/dogs/dog_113.jpg\n",
      "data/train/dogs/dog_47.jpg\n",
      "data/train/dogs/dog_90.jpg\n",
      "data/train/dogs/dog_305.jpg\n",
      "data/train/dogs/dog_84.jpg\n",
      "data/train/dogs/dog_473.jpg\n",
      "data/train/dogs/dog_315.jpg\n",
      "data/train/dogs/dog_80.jpg\n",
      "data/train/dogs/dog_301.jpg\n",
      "data/train/dogs/dog_57.jpg\n",
      "data/train/dogs/dog_103.jpg\n",
      "data/train/dogs/dog_261.jpg\n",
      "data/train/dogs/dog_513.jpg\n",
      "data/train/dogs/dog_249.jpg\n",
      "data/train/dogs/dog_512.jpg\n",
      "data/train/dogs/dog_260.jpg\n",
      "data/train/dogs/dog_102.jpg\n",
      "data/train/dogs/dog_116.jpg\n",
      "data/train/dogs/dog_56.jpg\n",
      "data/train/dogs/dog_466.jpg\n",
      "data/train/dogs/dog_97.jpg\n",
      "data/train/dogs/dog_458.jpg\n",
      "data/train/dogs/dog_54.jpg\n",
      "data/train/dogs/dog_100.jpg\n",
      "data/train/dogs/dog_128.jpg\n",
      "data/train/dogs/dog_262.jpg\n",
      "data/train/dogs/dog_504.jpg\n",
      "data/train/dogs/dog_505.jpg\n",
      "data/train/dogs/dog_277.jpg\n",
      "data/train/dogs/dog_129.jpg\n",
      "data/train/dogs/dog_69.jpg\n",
      "data/train/dogs/dog_82.jpg\n",
      "data/train/dogs/dog_317.jpg\n",
      "data/train/dogs/dog_471.jpg\n",
      "data/train/dogs/dog_358.jpg\n",
      "data/train/dogs/dog_402.jpg\n",
      "data/train/dogs/dog_32.jpg\n",
      "data/train/dogs/dog_26.jpg\n",
      "data/train/dogs/dog_199.jpg\n",
      "data/train/dogs/dog_238.jpg\n",
      "data/train/dogs/dog_576.jpg\n",
      "data/train/dogs/dog_204.jpg\n",
      "data/train/dogs/dog_589.jpg\n",
      "data/train/dogs/dog_205.jpg\n",
      "data/train/dogs/dog_239.jpg\n",
      "data/train/dogs/dog_167.jpg\n",
      "data/train/dogs/dog_33.jpg\n",
      "data/train/dogs/dog_371.jpg\n",
      "data/train/dogs/dog_7.jpg\n",
      "data/train/dogs/dog_417.jpg\n",
      "data/train/dogs/dog_403.jpg\n",
      "data/train/dogs/dog_365.jpg\n",
      "data/train/dogs/dog_359.jpg\n",
      "data/train/dogs/dog_429.jpg\n",
      "data/train/dogs/dog_367.jpg\n",
      "data/train/dogs/dog_401.jpg\n",
      "data/train/dogs/dog_31.jpg\n",
      "data/train/dogs/dog_171.jpg\n",
      "data/train/dogs/dog_165.jpg\n",
      "data/train/dogs/dog_549.jpg\n",
      "data/train/dogs/dog_207.jpg\n",
      "data/train/dogs/dog_212.jpg\n",
      "data/train/dogs/dog_560.jpg\n",
      "data/train/dogs/dog_206.jpg\n",
      "data/train/dogs/dog_164.jpg\n",
      "data/train/dogs/dog_30.jpg\n",
      "data/train/dogs/dog_414.jpg\n",
      "data/train/dogs/dog_4.jpg\n",
      "data/train/dogs/dog_428.jpg\n",
      "data/train/dogs/dog_376.jpg\n",
      "data/train/dogs/dog_0.jpg\n",
      "data/train/dogs/dog_404.jpg\n",
      "data/train/dogs/dog_438.jpg\n",
      "data/train/dogs/dog_389.jpg\n",
      "data/train/dogs/dog_202.jpg\n",
      "data/train/dogs/dog_564.jpg\n",
      "data/train/dogs/dog_570.jpg\n",
      "data/train/dogs/dog_216.jpg\n",
      "data/train/dogs/dog_558.jpg\n",
      "data/train/dogs/dog_559.jpg\n",
      "data/train/dogs/dog_571.jpg\n",
      "data/train/dogs/dog_217.jpg\n",
      "data/train/dogs/dog_203.jpg\n",
      "data/train/dogs/dog_175.jpg\n",
      "data/train/dogs/dog_35.jpg\n",
      "data/train/dogs/dog_21.jpg\n",
      "data/train/dogs/dog_405.jpg\n",
      "data/train/dogs/dog_363.jpg\n",
      "data/train/dogs/dog_361.jpg\n",
      "data/train/dogs/dog_407.jpg\n",
      "data/train/dogs/dog_375.jpg\n",
      "data/train/dogs/dog_3.jpg\n",
      "data/train/dogs/dog_349.jpg\n",
      "data/train/dogs/dog_37.jpg\n",
      "data/train/dogs/dog_23.jpg\n",
      "data/train/dogs/dog_188.jpg\n",
      "data/train/dogs/dog_566.jpg\n",
      "data/train/dogs/dog_200.jpg\n",
      "data/train/dogs/dog_214.jpg\n",
      "data/train/dogs/dog_572.jpg\n",
      "data/train/dogs/dog_162.jpg\n",
      "data/train/dogs/dog_22.jpg\n",
      "data/train/dogs/dog_412.jpg\n",
      "data/train/cats/cat_386.jpg\n",
      "data/train/cats/cat_15.jpg\n",
      "data/train/cats/cat_345.jpg\n",
      "data/train/cats/cat_423.jpg\n",
      "data/train/cats/cat_351.jpg\n",
      "data/train/cats/cat_184.jpg\n",
      "data/train/cats/cat_153.jpg\n",
      "data/train/cats/cat_580.jpg\n",
      "data/train/cats/cat_557.jpg\n",
      "data/train/cats/cat_543.jpg\n",
      "data/train/cats/cat_225.jpg\n",
      "data/train/cats/cat_219.jpg\n",
      "data/train/cats/cat_218.jpg\n",
      "data/train/cats/cat_224.jpg\n",
      "data/train/cats/cat_230.jpg\n",
      "data/train/cats/cat_556.jpg\n",
      "data/train/cats/cat_581.jpg\n",
      "data/train/cats/cat_152.jpg\n",
      "data/train/cats/cat_146.jpg\n",
      "data/train/cats/cat_191.jpg\n",
      "data/train/cats/cat_350.jpg\n",
      "data/train/cats/cat_344.jpg\n",
      "data/train/cats/cat_393.jpg\n",
      "data/train/cats/cat_387.jpg\n",
      "data/train/cats/cat_14.jpg\n",
      "data/train/cats/cat_391.jpg\n",
      "data/train/cats/cat_16.jpg\n",
      "data/train/cats/cat_385.jpg\n",
      "data/train/cats/cat_352.jpg\n",
      "data/train/cats/cat_434.jpg\n",
      "data/train/cats/cat_420.jpg\n",
      "data/train/cats/cat_408.jpg\n",
      "data/train/cats/cat_540.jpg\n",
      "data/train/cats/cat_232.jpg\n",
      "data/train/cats/cat_569.jpg\n",
      "data/train/cats/cat_227.jpg\n",
      "data/train/cats/cat_179.jpg\n",
      "data/train/cats/cat_151.jpg\n",
      "data/train/cats/cat_435.jpg\n",
      "data/train/cats/cat_380.jpg\n",
      "data/train/cats/cat_13.jpg\n",
      "data/train/cats/cat_419.jpg\n",
      "data/train/cats/cat_343.jpg\n",
      "data/train/cats/cat_169.jpg\n",
      "data/train/cats/cat_141.jpg\n",
      "data/train/cats/cat_592.jpg\n",
      "data/train/cats/cat_544.jpg\n",
      "data/train/cats/cat_222.jpg\n",
      "data/train/cats/cat_168.jpg\n",
      "data/train/cats/cat_183.jpg\n",
      "data/train/cats/cat_381.jpg\n",
      "data/train/cats/cat_12.jpg\n",
      "data/train/cats/cat_38.jpg\n",
      "data/train/cats/cat_10.jpg\n",
      "data/train/cats/cat_397.jpg\n",
      "data/train/cats/cat_426.jpg\n",
      "data/train/cats/cat_195.jpg\n",
      "data/train/cats/cat_591.jpg\n",
      "data/train/cats/cat_208.jpg\n",
      "data/train/cats/cat_220.jpg\n",
      "data/train/cats/cat_546.jpg\n",
      "data/train/cats/cat_221.jpg\n",
      "data/train/cats/cat_547.jpg\n",
      "data/train/cats/cat_553.jpg\n",
      "data/train/cats/cat_235.jpg\n",
      "data/train/cats/cat_209.jpg\n",
      "data/train/cats/cat_590.jpg\n",
      "data/train/cats/cat_194.jpg\n",
      "data/train/cats/cat_180.jpg\n",
      "data/train/cats/cat_427.jpg\n",
      "data/train/cats/cat_369.jpg\n",
      "data/train/cats/cat_396.jpg\n",
      "data/train/cats/cat_11.jpg\n",
      "data/train/cats/cat_39.jpg\n",
      "data/train/cats/cat_483.jpg\n",
      "data/train/cats/cat_497.jpg\n",
      "data/train/cats/cat_440.jpg\n",
      "data/train/cats/cat_285.jpg\n",
      "data/train/cats/cat_246.jpg\n",
      "data/train/cats/cat_508.jpg\n",
      "data/train/cats/cat_9.jpg\n",
      "data/train/cats/cat_247.jpg\n",
      "data/train/cats/cat_535.jpg\n",
      "data/train/cats/cat_253.jpg\n",
      "data/train/cats/cat_284.jpg\n",
      "data/train/cats/cat_469.jpg\n",
      "data/train/cats/cat_333.jpg\n",
      "data/train/cats/cat_455.jpg\n",
      "data/train/cats/cat_327.jpg\n",
      "data/train/cats/cat_77.jpg\n",
      "data/train/cats/cat_482.jpg\n",
      "data/train/cats/cat_61.jpg\n",
      "data/train/cats/cat_494.jpg\n",
      "data/train/cats/cat_75.jpg\n",
      "data/train/cats/cat_49.jpg\n",
      "data/train/cats/cat_331.jpg\n",
      "data/train/cats/cat_325.jpg\n",
      "data/train/cats/cat_443.jpg\n",
      "data/train/cats/cat_133.jpg\n",
      "data/train/cats/cat_292.jpg\n",
      "data/train/cats/cat_536.jpg\n",
      "data/train/cats/cat_293.jpg\n",
      "data/train/cats/cat_132.jpg\n",
      "data/train/cats/cat_318.jpg\n",
      "data/train/cats/cat_456.jpg\n",
      "data/train/cats/cat_330.jpg\n",
      "data/train/cats/cat_74.jpg\n",
      "data/train/cats/cat_495.jpg\n",
      "data/train/cats/cat_58.jpg\n",
      "data/train/cats/cat_64.jpg\n",
      "data/train/cats/cat_485.jpg\n",
      "data/train/cats/cat_70.jpg\n",
      "data/train/cats/cat_452.jpg\n",
      "data/train/cats/cat_320.jpg\n",
      "data/train/cats/cat_297.jpg\n",
      "data/train/cats/cat_240.jpg\n",
      "data/train/cats/cat_526.jpg\n",
      "data/train/cats/cat_532.jpg\n",
      "data/train/cats/cat_533.jpg\n",
      "data/train/cats/cat_269.jpg\n",
      "data/train/cats/cat_123.jpg\n",
      "data/train/cats/cat_137.jpg\n",
      "data/train/cats/cat_321.jpg\n",
      "data/train/cats/cat_453.jpg\n",
      "data/train/cats/cat_490.jpg\n",
      "data/train/cats/cat_59.jpg\n",
      "data/train/cats/cat_67.jpg\n",
      "data/train/cats/cat_492.jpg\n",
      "data/train/cats/cat_98.jpg\n",
      "data/train/cats/cat_445.jpg\n",
      "data/train/cats/cat_280.jpg\n",
      "data/train/cats/cat_519.jpg\n",
      "data/train/cats/cat_257.jpg\n",
      "data/train/cats/cat_531.jpg\n",
      "data/train/cats/cat_243.jpg\n",
      "data/train/cats/cat_524.jpg\n",
      "data/train/cats/cat_518.jpg\n",
      "data/train/cats/cat_134.jpg\n",
      "data/train/cats/cat_120.jpg\n",
      "data/train/cats/cat_450.jpg\n",
      "data/train/cats/cat_336.jpg\n",
      "data/train/cats/cat_493.jpg\n",
      "data/train/cats/cat_66.jpg\n",
      "data/train/cats/cat_487.jpg\n",
      "data/train/cats/cat_43.jpg\n",
      "data/train/cats/cat_461.jpg\n",
      "data/train/cats/cat_307.jpg\n",
      "data/train/cats/cat_80.jpg\n",
      "data/train/cats/cat_111.jpg\n",
      "data/train/cats/cat_139.jpg\n",
      "data/train/cats/cat_298.jpg\n",
      "data/train/cats/cat_273.jpg\n",
      "data/train/cats/cat_500.jpg\n",
      "data/train/cats/cat_138.jpg\n",
      "data/train/cats/cat_104.jpg\n",
      "data/train/cats/cat_474.jpg\n",
      "data/train/cats/cat_95.jpg\n",
      "data/train/cats/cat_42.jpg\n",
      "data/train/cats/cat_54.jpg\n",
      "data/train/cats/cat_489.jpg\n",
      "data/train/cats/cat_310.jpg\n",
      "data/train/cats/cat_304.jpg\n",
      "data/train/cats/cat_97.jpg\n",
      "data/train/cats/cat_270.jpg\n",
      "data/train/cats/cat_2.jpg\n",
      "data/train/cats/cat_271.jpg\n",
      "data/train/cats/cat_3.jpg\n",
      "data/train/cats/cat_265.jpg\n",
      "data/train/cats/cat_463.jpg\n",
      "data/train/cats/cat_311.jpg\n",
      "data/train/cats/cat_69.jpg\n",
      "data/train/cats/cat_498.jpg\n",
      "data/train/cats/cat_329.jpg\n",
      "data/train/cats/cat_467.jpg\n",
      "data/train/cats/cat_103.jpg\n",
      "data/train/cats/cat_261.jpg\n",
      "data/train/cats/cat_275.jpg\n",
      "data/train/cats/cat_6.jpg\n",
      "data/train/cats/cat_274.jpg\n",
      "data/train/cats/cat_260.jpg\n",
      "data/train/cats/cat_506.jpg\n",
      "data/train/cats/cat_248.jpg\n",
      "data/train/cats/cat_102.jpg\n",
      "data/train/cats/cat_93.jpg\n",
      "data/train/cats/cat_466.jpg\n",
      "data/train/cats/cat_87.jpg\n",
      "data/train/cats/cat_50.jpg\n",
      "data/train/cats/cat_499.jpg\n",
      "data/train/cats/cat_78.jpg\n",
      "data/train/cats/cat_52.jpg\n",
      "data/train/cats/cat_46.jpg\n",
      "data/train/cats/cat_316.jpg\n",
      "data/train/cats/cat_100.jpg\n",
      "data/train/cats/cat_289.jpg\n",
      "data/train/cats/cat_276.jpg\n",
      "data/train/cats/cat_4.jpg\n",
      "data/train/cats/cat_510.jpg\n",
      "data/train/cats/cat_505.jpg\n",
      "data/train/cats/cat_263.jpg\n",
      "data/train/cats/cat_511.jpg\n",
      "data/train/cats/cat_101.jpg\n",
      "data/train/cats/cat_129.jpg\n",
      "data/train/cats/cat_84.jpg\n",
      "data/train/cats/cat_317.jpg\n",
      "data/train/cats/cat_303.jpg\n",
      "data/train/cats/cat_465.jpg\n",
      "data/train/cats/cat_53.jpg\n",
      "data/train/cats/cat_34.jpg\n",
      "data/train/cats/cat_402.jpg\n",
      "data/train/cats/cat_416.jpg\n",
      "data/train/cats/cat_370.jpg\n",
      "data/train/cats/cat_172.jpg\n",
      "data/train/cats/cat_589.jpg\n",
      "data/train/cats/cat_238.jpg\n",
      "data/train/cats/cat_211.jpg\n",
      "data/train/cats/cat_577.jpg\n",
      "data/train/cats/cat_601.jpg\n",
      "data/train/cats/cat_198.jpg\n",
      "data/train/cats/cat_359.jpg\n",
      "data/train/cats/cat_403.jpg\n",
      "data/train/cats/cat_21.jpg\n",
      "data/train/cats/cat_35.jpg\n",
      "data/train/cats/cat_37.jpg\n",
      "data/train/cats/cat_373.jpg\n",
      "data/train/cats/cat_367.jpg\n",
      "data/train/cats/cat_429.jpg\n",
      "data/train/cats/cat_165.jpg\n",
      "data/train/cats/cat_159.jpg\n",
      "data/train/cats/cat_207.jpg\n",
      "data/train/cats/cat_213.jpg\n",
      "data/train/cats/cat_549.jpg\n",
      "data/train/cats/cat_548.jpg\n",
      "data/train/cats/cat_560.jpg\n",
      "data/train/cats/cat_164.jpg\n",
      "data/train/cats/cat_602.jpg\n",
      "data/train/cats/cat_170.jpg\n",
      "data/train/cats/cat_428.jpg\n",
      "data/train/cats/cat_400.jpg\n",
      "data/train/cats/cat_372.jpg\n",
      "data/train/cats/cat_414.jpg\n",
      "data/train/cats/cat_399.jpg\n",
      "data/train/cats/cat_26.jpg\n",
      "data/train/cats/cat_438.jpg\n",
      "data/train/cats/cat_376.jpg\n",
      "data/train/cats/cat_362.jpg\n",
      "data/train/cats/cat_148.jpg\n",
      "data/train/cats/cat_160.jpg\n",
      "data/train/cats/cat_558.jpg\n",
      "data/train/cats/cat_202.jpg\n",
      "data/train/cats/cat_570.jpg\n",
      "data/train/cats/cat_217.jpg\n",
      "data/train/cats/cat_565.jpg\n",
      "data/train/cats/cat_559.jpg\n",
      "data/train/cats/cat_161.jpg\n",
      "data/train/cats/cat_149.jpg\n",
      "data/train/cats/cat_363.jpg\n",
      "data/train/cats/cat_405.jpg\n",
      "data/train/cats/cat_377.jpg\n",
      "data/train/cats/cat_33.jpg\n",
      "data/train/cats/cat_27.jpg\n",
      "data/train/cats/cat_388.jpg\n",
      "data/train/cats/cat_407.jpg\n",
      "data/train/cats/cat_361.jpg\n",
      "data/train/cats/cat_413.jpg\n",
      "data/train/cats/cat_163.jpg\n",
      "data/train/cats/cat_605.jpg\n",
      "data/train/cats/cat_177.jpg\n",
      "data/train/cats/cat_229.jpg\n",
      "data/train/cats/cat_573.jpg\n",
      "data/train/cats/cat_215.jpg\n",
      "data/train/cats/cat_201.jpg\n",
      "data/train/cats/cat_567.jpg\n",
      "data/train/cats/cat_200.jpg\n",
      "data/train/cats/cat_214.jpg\n",
      "data/train/cats/cat_228.jpg\n",
      "data/train/cats/cat_604.jpg\n",
      "data/train/cats/cat_189.jpg\n",
      "data/train/cats/cat_348.jpg\n",
      "data/train/cats/cat_24.jpg\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for dirname, _, filenames in os.walk('data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Data augmentation kept in tact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# 1. Data Augmentation\n",
    "# ==========================\n",
    "image_size = (32, 32)  # was originally (128, 128)\n",
    "batch_size = 32\n",
    "train_dir = \"data/train\"\n",
    "test_dir = \"data/test\"\n",
    "\n",
    "# Define transforms (like ImageDataGenerator in Keras)\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5], [0.5, 0.5])  # rescale to [-1,1]\n",
    "])\n",
    "\n",
    "val_test_transforms = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5], [0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Load datasets with ImageFolder\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transforms)\n",
    "val_size = int(0.2 * len(train_dataset))\n",
    "train_size = len(train_dataset) - val_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=val_test_transforms)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# # 2. Model Definition\n",
    "# # ==========================\n",
    "# class SimpleNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(SimpleNN, self).__init__()\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.fc1 = nn.Linear(128*128*3, 256)\n",
    "#         self.fc2 = nn.Linear(256, 128)\n",
    "#         self.fc3 = nn.Linear(128, 1)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.flatten(x)\n",
    "#         x = self.relu(self.fc1(x))\n",
    "#         x = self.relu(self.fc2(x))\n",
    "#         x = self.sigmoid(self.fc3(x))\n",
    "#         return x\n",
    "\n",
    "# model = SimpleNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition of AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# 2. Hebbian Model (Oja’s Rule)\n",
    "# ==========================\n",
    "class OjaLayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size, eta=0.001):\n",
    "        super(OjaLayer, self).__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(output_size, input_size) * 0.01, requires_grad=False)\n",
    "        self.eta = eta\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, input_size]\n",
    "        y = x @ self.weights.t()  # [batch, output_size]\n",
    "        return y\n",
    "\n",
    "    def oja_update(self, x):\n",
    "        x = x - x.mean(dim=0, keepdim=True)  # center inputs\n",
    "        y = x @ self.weights.t()\n",
    "        for i in range(y.size(0)):\n",
    "            xi = x[i].unsqueeze(0)\n",
    "            yi = y[i].unsqueeze(1)\n",
    "            dw = self.eta * (yi @ (xi - yi.t() @ self.weights))\n",
    "            self.weights.data += dw\n",
    "\n",
    "class HebbianNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HebbianNet, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.hebb1 = OjaLayer(32*32*3, 128, eta=0.01)  \n",
    "        self.fc = nn.Linear(128, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.hebb1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original compile setup and model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ==========================\n",
    "# # 3. Compile setup\n",
    "# # ==========================\n",
    "# criterion = nn.BCELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # ==========================\n",
    "# # 4. Train the model\n",
    "# # ==========================\n",
    "# if torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"mps\")\n",
    "# elif torch.cuda.is_available():\n",
    "#     device = torch.device(\"cuda\")\n",
    "# else:\n",
    "#     device = torch.device(\"cpu\")\n",
    "\n",
    "# model.to(device)\n",
    "\n",
    "# epochs = 10\n",
    "# for epoch in range(epochs):\n",
    "#     model.train()\n",
    "#     running_loss, correct, total = 0.0, 0, 0\n",
    "#     for inputs, labels in train_loader:\n",
    "#         inputs, labels = inputs.to(device), labels.float().to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(inputs).squeeze()  # shape [batch_size]\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item() * inputs.size(0)\n",
    "#         preds = (outputs > 0.5).float()\n",
    "#         correct += (preds == labels).sum().item()\n",
    "#         total += labels.size(0)\n",
    "\n",
    "#     train_acc = 100 * correct / total\n",
    "#     train_loss = running_loss / total\n",
    "\n",
    "#     # Validation\n",
    "#     model.eval()\n",
    "#     val_correct, val_total, val_loss = 0, 0, 0.0\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, labels in val_loader:\n",
    "#             inputs, labels = inputs.to(device), labels.float().to(device)\n",
    "#             outputs = model(inputs).squeeze()\n",
    "#             loss = criterion(outputs, labels)\n",
    "\n",
    "#             val_loss += loss.item() * inputs.size(0)\n",
    "#             preds = (outputs > 0.5).float()\n",
    "#             val_correct += (preds == labels).sum().item()\n",
    "#             val_total += labels.size(0)\n",
    "\n",
    "#     val_acc = 100 * val_correct / val_total\n",
    "#     val_loss = val_loss / val_total\n",
    "\n",
    "#     print(f\"Epoch [{epoch+1}/{epochs}] \"\n",
    "#           f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% \"\n",
    "#           f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Setting and model training according to chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# 3. Training Loop with Oja’s Learning\n",
    "# ==========================\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model = HebbianNet().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)  # only train last layer via gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m model.train()\n\u001b[32m      4\u001b[39m running_loss, correct, total = \u001b[32m0.0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Oja update (unsupervised Hebbian)\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/Minor/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/Minor/lib/python3.12/site-packages/torch/utils/data/dataloader.py:757\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    756\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    759\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/Minor/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/Minor/lib/python3.12/site-packages/torch/utils/data/dataset.py:420\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    418\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/Minor/lib/python3.12/site-packages/torchvision/datasets/folder.py:247\u001b[39m, in \u001b[36mDatasetFolder.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    245\u001b[39m sample = \u001b[38;5;28mself\u001b[39m.loader(path)\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     sample = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.target_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    249\u001b[39m     target = \u001b[38;5;28mself\u001b[39m.target_transform(target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/Minor/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/Minor/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/Minor/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/Minor/lib/python3.12/site-packages/torchvision/transforms/transforms.py:277\u001b[39m, in \u001b[36mNormalize.forward\u001b[39m\u001b[34m(self, tensor)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) -> Tensor:\n\u001b[32m    270\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    271\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    272\u001b[39m \u001b[33;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    275\u001b[39m \u001b[33;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[32m    276\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/Minor/lib/python3.12/site-packages/torchvision/transforms/functional.py:350\u001b[39m, in \u001b[36mnormalize\u001b[39m\u001b[34m(tensor, mean, std, inplace)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch.Tensor):\n\u001b[32m    348\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/Minor/lib/python3.12/site-packages/torchvision/transforms/_functional_tensor.py:928\u001b[39m, in \u001b[36mnormalize\u001b[39m\u001b[34m(tensor, mean, std, inplace)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m std.ndim == \u001b[32m1\u001b[39m:\n\u001b[32m    927\u001b[39m     std = std.view(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43msub_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m)\u001b[49m.div_(std)\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.float().to(device)\n",
    "\n",
    "        # Oja update (unsupervised Hebbian)\n",
    "        flat_inputs = inputs.view(inputs.size(0), -1)\n",
    "        model.hebb1.oja_update(flat_inputs)\n",
    "\n",
    "        # Standard supervised pass for output layer\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        preds = (outputs > 0.5).float()\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_loss = running_loss / total\n",
    "    train_acc = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "\n",
    "\n",
    "    if (epoch + 1) % 2 == 0:  # visualize every 2 epochs\n",
    "        w = model.hebb1.weights.cpu().detach().numpy()\n",
    "        w_min, w_max = w.min(), w.max()\n",
    "        w = (w - w_min) / (w_max - w_min)\n",
    "        num_filters = min(8, w.shape[0])\n",
    "        cols = 4\n",
    "        rows = int(np.ceil(num_filters / cols))\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        for i in range(num_filters):\n",
    "            plt.subplot(rows, cols, i + 1)\n",
    "            # plt.imshow(w[i].reshape(32, 32, 3))\n",
    "            plt.imshow(w[i].reshape(32, 32), cmap='gray', vmin=0, vmax=1)\n",
    "            plt.axis('off')\n",
    "        plt.suptitle(f\"Epoch {epoch+1}: Hebbian Filters\")\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
