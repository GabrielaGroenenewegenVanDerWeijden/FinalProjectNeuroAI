{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final NeuroAI Project\n",
    "## Group 1: Femke, Tikva and Gabriela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing everything important. \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc as sp\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the standard seed for reproducable results.\n",
    "random.seed(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loads the images from the data set in black and white. While keeping the ratio of cats and dogs equal \n",
    "# and keeping track of the picture labels. \n",
    "def load_balanced_images(folder, image_size=(32, 32), n_per_class=None):\n",
    "    \"\"\"Load the same number of grayscale images per class (e.g. cats/dogs).\"\"\"\n",
    "    classes = sorted(os.listdir(folder))  # ['cats', 'dogs']\n",
    "    all_images, all_labels = [], []\n",
    "    \n",
    "    for label in classes:\n",
    "        class_folder = os.path.join(folder, label)\n",
    "        paths = []\n",
    "        for ext in ('*.png', '*.jpg', '*.jpeg'):\n",
    "            paths.extend(glob(os.path.join(class_folder, ext)))\n",
    "        random.shuffle(paths)\n",
    "        \n",
    "        if n_per_class:\n",
    "            paths = paths[:n_per_class]  # limit per class\n",
    "        \n",
    "        # Going through each image and giving it an index.\n",
    "        for idx, path in enumerate(paths):\n",
    "            img = Image.open(path).convert('L').resize(image_size)\n",
    "            img = np.array(img, dtype=np.float32) / 255.0\n",
    "            all_images.append(img)\n",
    "            all_labels.append(f\"{label}_{idx+1}\")  # UNIQUE label, e.g. cat_1, dog_3\n",
    "    \n",
    "    # Mix the dataset\n",
    "    combined = list(zip(all_images, all_labels))\n",
    "    random.shuffle(combined)\n",
    "    all_images, all_labels = zip(*combined)\n",
    "    \n",
    "    print(f\"Loaded {len(all_images)} images from {folder} \"\n",
    "          f\"({len(classes)} classes, {n_per_class} per class)\")\n",
    "    \n",
    "    return list(all_images), list(all_labels)\n",
    "\n",
    "\n",
    "# This code generates the noisy picture.\n",
    "def imageGenerator(imageVector, binary=None):\n",
    "    imageVector = imageVector.astype(float)\n",
    "    imageVector /= np.max(imageVector)\n",
    "    # If binary is set to True, then the image gets binarised. Otherwise (else) it is not, thus staying in grayscale.\n",
    "    if binary: \n",
    "        cleanImage = np.where(imageVector > 0.5, 1, -1)\n",
    "        noisyImage = cleanImage + np.random.normal(0, 0.8, cleanImage.shape)\n",
    "        noisyImage = np.where(noisyImage >= 0, 1, -1)\n",
    "    else: \n",
    "        # Continuous case\n",
    "        cleanImage = 2 * (imageVector / np.max(imageVector)) - 1  # in [-1, 1]\n",
    "        noisyImage = cleanImage + np.random.normal(0, 0.15, cleanImage.shape) # Making it less noisier when grayscale, otherwise totally random picture with white/black dots.\n",
    "        noisyImage = np.clip(noisyImage, -1, 1)  # keep within range, no binarization\n",
    "    return cleanImage, noisyImage\n",
    "\n",
    "\n",
    "# Here happens they hebbian learning. \n",
    "def hebbian_trainer(pattern, old_weights=None):\n",
    "    \"\"\"\n",
    "    Simple Hebbian learning rule for Hopfield network.\n",
    "    \n",
    "    pattern : 1D numpy array of Â±1 values\n",
    "    weights : existing weight matrix or None (for first pattern)\n",
    "    \"\"\"\n",
    "    # Flatten the image to get the pattern.\n",
    "    pattern = pattern.flatten().astype(np.float32)\n",
    "    N = len(pattern)\n",
    "\n",
    "    # Inititalizing new_weights with all zeros.\n",
    "    new_weights = np.zeros((N, N))\n",
    "\n",
    "    # --- Hebbian learning ---\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if i != j:  # no self-connection\n",
    "                new_weights[i, j] = pattern[i] * pattern[j]\n",
    "\n",
    "    # Normalize by vector length to avoid huge activations, it prevents saturation.\n",
    "    new_weights /= N\n",
    "\n",
    "    # If its the first iteration the new weigths are the first weights otherwise we update the weight matrix.\n",
    "    if old_weights is None:\n",
    "        return new_weights\n",
    "    else:\n",
    "        return old_weights + new_weights\n",
    "\n",
    "# This returns the pattern.\n",
    "def prediction(corruptedVec, coefMat, binary):\n",
    "    \"\"\"Hopfield recall: one synchronous update.\"\"\"\n",
    "    corruptedVec = corruptedVec.flatten()\n",
    "    \n",
    "    # If binary, we want to recall a binary pattern. Otherwise grayscale difference in sign and tanh function.\n",
    "    if binary:\n",
    "        predictVec = np.sign(np.dot(coefMat, corruptedVec))\n",
    "        predictVec[predictVec == 0] = 1  # handle zeros\n",
    "    else: \n",
    "        predictVec = np.tanh(np.dot(coefMat, corruptedVec))\n",
    "\n",
    "    side = int(np.sqrt(len(predictVec)))\n",
    "    return predictVec.reshape((side, side))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This plots the four pictures side by side. (The original, the cleaned, the noisy and the recalled.)\n",
    "def plot_image_reconstruction(selected_images, selected_labels, coefMatrix, imageGenerator, prediction, save_path=None, binary=None):\n",
    "\n",
    "    n_images = len(selected_images)\n",
    "    plt.figure(figsize=(15, 5 * n_images))\n",
    "\n",
    "    for i, img in enumerate(selected_images):\n",
    "        clean, noisyVec = imageGenerator(img, binary)\n",
    "        predictedVec = prediction(noisyVec, coefMatrix, binary)\n",
    "\n",
    "        # Original \n",
    "        plt.subplot(n_images, 4, 4 * i + 1)\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.title(f'Original ({selected_labels[i]})')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Cleaned \n",
    "        plt.subplot(n_images, 4, 4 * i + 2)\n",
    "        plt.imshow(clean, cmap='gray')\n",
    "        plt.title('Cleaned')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Noisy\n",
    "        plt.subplot(n_images, 4, 4 * i + 3)\n",
    "        plt.imshow(noisyVec, cmap='gray')\n",
    "        plt.title('Noisy')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Recalled\n",
    "        plt.subplot(n_images, 4, 4 * i + 4)\n",
    "        if binary: \n",
    "            plt.imshow(predictedVec, cmap='gray')\n",
    "        else: \n",
    "            plt.imshow((predictedVec + 1) / 2, cmap='gray')\n",
    "        plt.title('Recalled')\n",
    "        plt.axis('off')\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Saving the image\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# This creates the confusion matrix.\n",
    "def plot_internal_recall_confusion(\n",
    "        learning_rule,\n",
    "        image_size,\n",
    "        stored_patterns,\n",
    "        stored_labels,\n",
    "        coefMatrix,\n",
    "        imageGenerator,\n",
    "        prediction,\n",
    "        binary,\n",
    "        n_samples=10,\n",
    "        color = None,\n",
    "        save_path=None\n",
    "    ):\n",
    "    idxs = np.random.choice(len(stored_patterns), n_samples, replace=False)\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    for i in idxs:\n",
    "        clean = stored_patterns[i]\n",
    "        noisy_img = imageGenerator(clean, binary)[1]\n",
    "        recalled = prediction(noisy_img, coefMatrix, binary)\n",
    "\n",
    "        # Restrict to sampled subset\n",
    "        similarities = [np.mean(recalled == stored_patterns[j]) for j in idxs]\n",
    "        best_match_local = np.argmax(similarities)\n",
    "        best_match_idx = idxs[best_match_local]\n",
    "\n",
    "        y_true.append(i)\n",
    "        y_pred.append(best_match_idx)\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = np.mean(np.array(y_true) == np.array(y_pred))\n",
    "    print(f\"Internal recall accuracy ({n_samples} samples): {accuracy:.3f}\")\n",
    "\n",
    "    # --- Label names for the selected subset ---\n",
    "    labels = [stored_labels[i] for i in idxs]\n",
    "\n",
    "    # --- Confusion matrix ---\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=idxs)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap=color, values_format='d', colorbar=False)\n",
    "\n",
    "    plt.title(f\"Model {learning_rule}, binary:{binary}, neurons:{image_size},(accuracy={accuracy:.2f}), patterns={len(stored_patterns)}\", fontsize=14)\n",
    "\n",
    "    # Rotate and clean up label text\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=8)\n",
    "    plt.yticks(rotation=0, fontsize=8)\n",
    "    plt.xlabel(\"Predicted pattern\", fontsize=10)\n",
    "    plt.ylabel(\"True pattern\", fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    return accuracy, cm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hopfield_experiment(train_dir=\"data/train\", train_counts=None, \n",
    "                            learning_rule=\"hebbian\", hebbian_trainer=None, color = \"Blues\",\n",
    "                            imageGenerator=None, prediction=None, plot_image_reconstruction=None, image_size=(32, 32), binary=None):\n",
    "   \n",
    "    if train_counts is None:\n",
    "        train_counts = list(range(1, 13))\n",
    "\n",
    "    # --- Load images ---\n",
    "    train_images, train_labels = load_balanced_images(\"data/train\", image_size=image_size, n_per_class=75)\n",
    "   \n",
    "    results = []\n",
    "    conf_matrices = []\n",
    "    train_labels_list = []\n",
    "\n",
    "    for n_train in train_counts:\n",
    "        print(f\"\\n=== Training with {n_train} patterns ===\")\n",
    "        selected_indices = random.sample(range(len(train_images)), n_train)\n",
    "        selected_images = [train_images[i] for i in selected_indices]\n",
    "        selected_labels = [train_labels[i] for i in selected_indices]\n",
    "\n",
    "        coefMatrix = 0\n",
    "        stored_patterns = []\n",
    "\n",
    "        # --- Learning phase ---\n",
    "        for i, img in enumerate(selected_images):\n",
    "            clean, noisyVec = imageGenerator(img, binary)\n",
    "\n",
    "            if learning_rule.lower() == \"hebbian\":\n",
    "                coefMatrix = hebbian_trainer(clean, 0 if i == 0 else coefMatrix)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid learning_rule. Choose 'hebbian'.\")\n",
    "\n",
    "            stored_patterns.append(clean)\n",
    "\n",
    "        # --- Visualization ---\n",
    "        plot_image_reconstruction(\n",
    "            selected_images=selected_images,\n",
    "            selected_labels=selected_labels,\n",
    "            coefMatrix=coefMatrix,\n",
    "            imageGenerator=imageGenerator,\n",
    "            prediction=prediction,\n",
    "            save_path=f\"hopfield_stage_{learning_rule}_{n_train}_binary{binary}_neurons{image_size}.png\",\n",
    "            binary = binary\n",
    "        )\n",
    "\n",
    "        acc, cm = plot_internal_recall_confusion(\n",
    "            image_size=image_size,\n",
    "            learning_rule=learning_rule,\n",
    "            stored_patterns=stored_patterns,\n",
    "            stored_labels=selected_labels,\n",
    "            coefMatrix=coefMatrix,\n",
    "            imageGenerator=imageGenerator,\n",
    "            prediction=prediction,\n",
    "            binary=binary,\n",
    "            n_samples=len(stored_patterns),\n",
    "            color = color,\n",
    "            save_path=f\"internal_confusion_{learning_rule}_{n_train}_binary{binary}_neurons{image_size}.png\"\n",
    "        )\n",
    "\n",
    "        conf_matrices.append(cm)\n",
    "        results.append(acc)\n",
    "        train_labels_list.append(n_train)\n",
    "        print(f\"Internal recall accuracy with {n_train} training images (binary: {binary}, rule: {learning_rule}, neurons: {image_size}): {acc:.2f}\")\n",
    "\n",
    "    return results, conf_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the training counts\n",
    "train_counts = [10, 20 , 30, 40, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Base model\n",
    "results_hebbian, confs_hebbian = run_hopfield_experiment(\n",
    "    learning_rule=\"hebbian\",\n",
    "    hebbian_trainer=hebbian_trainer,\n",
    "    train_counts= train_counts,\n",
    "    color = \"Blues\", # Confusin matrix color\n",
    "    imageGenerator=imageGenerator,\n",
    "    prediction=prediction,\n",
    "    plot_image_reconstruction=plot_image_reconstruction,\n",
    "    image_size=(32, 32), # Neurons ammount\n",
    "    binary = True # Black/White or grayscale\n",
    ")\n",
    "\n",
    "np.savez(\"results_hebbian.npz\", results=results_hebbian, confs=confs_hebbian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base model with non-binary\n",
    "results_non_binary, confs_non_binary = run_hopfield_experiment(\n",
    "    learning_rule=\"hebbian\",\n",
    "    hebbian_trainer=hebbian_trainer,\n",
    "    train_counts= train_counts,\n",
    "    color=plt.cm.RdPu,\n",
    "    imageGenerator=imageGenerator,\n",
    "    prediction=prediction,\n",
    "    plot_image_reconstruction=plot_image_reconstruction,\n",
    "    image_size=(64, 64),\n",
    "    binary= False # Continuous values\n",
    ")\n",
    "\n",
    "np.savez(\"results_non_binary.npz\", results=results_non_binary, confs=confs_non_binary, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base model with more neurons\n",
    "results_more_neurons, confs_more_neurons = run_hopfield_experiment(\n",
    "    learning_rule=\"hebbian\",\n",
    "    hebbian_trainer=hebbian_trainer,\n",
    "    train_counts= train_counts,\n",
    "    color=plt.cm.RdPu, # Confusion matrix color\n",
    "    imageGenerator=imageGenerator,\n",
    "    prediction=prediction,\n",
    "    plot_image_reconstruction=plot_image_reconstruction,\n",
    "    image_size=(128, 128), # More neurons\n",
    "    binary = True # Black/white or grayscale\n",
    ")\n",
    "\n",
    "np.savez(\"results_more_neurons.npz\", results=results_more_neurons, confs=confs_more_neurons, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting learning improvement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the saved data.\n",
    "data_hebbian = np.load(\"results_hebbian.npz\", allow_pickle=True)\n",
    "data_more_neurons = np.load(\"results_more_neurons.npz\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the results from the loaded data.\n",
    "results_hebbian = data_hebbian[\"results\"]\n",
    "results_more_neurons = data_more_neurons[\"results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you do not want to weight 4 hours training (here are the results, uncomment them.)\n",
    "# results_hebbian = [1.0, 0.45, 0.33, 0.25, 0.16]\n",
    "# results_more_neurons = [0.9, 0.4, 0.1, 0.15, 0.12]\n",
    "# train_counts = [10, 20, 30, 40, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_hebbian = \"#9b5de5\"      # violet-purple\n",
    "color_more_neurons = \"#f15bb5\" # hot pink\n",
    "\n",
    "# --- Create figure with subplots ---\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
    "axes = axes.flatten()  # make it easy to index as a 1D array\n",
    "\n",
    "# --- Plot 1: Base model ---\n",
    "axes[0].plot(train_counts, results_hebbian, 'o-', color=color_hebbian)\n",
    "axes[0].set_title(\"Base Hebbian model\")\n",
    "axes[0].set_xlabel(\"Training images\")\n",
    "axes[0].set_ylabel(\"Accuracy\")\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[0].set_xticks(train_counts)\n",
    "axes[0].set_yticks(np.arange(0.1, 1.1, 0.1))\n",
    "axes[0].grid(True)\n",
    "\n",
    "# --- Plot 2: More neurons model ---\n",
    "axes[1].plot(train_counts, results_more_neurons, 's-', color=color_more_neurons)\n",
    "axes[1].set_title(\"More neurons model\")\n",
    "axes[1].set_xlabel(\"Training images\")\n",
    "axes[1].set_ylabel(\"Accuracy\")\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].set_xticks(train_counts)\n",
    "axes[1].set_yticks(np.arange(0.1, 1.1, 0.1))\n",
    "axes[1].grid(True)\n",
    "\n",
    "# --- Plot 3: Combined comparison ---\n",
    "axes[2].plot(train_counts, results_hebbian, 'o-', color=color_hebbian, label='Base model')\n",
    "axes[2].plot(train_counts, results_more_neurons, 's-', color=color_more_neurons, label='More neurons')\n",
    "axes[2].set_title(\"All models compared\")\n",
    "axes[2].set_xlabel(\"Training images\")\n",
    "axes[2].set_ylabel(\"Accuracy\")\n",
    "axes[2].set_ylim(0, 1)\n",
    "axes[2].set_xticks(train_counts)\n",
    "axes[2].set_yticks(np.arange(0.1, 1.1, 0.1))\n",
    "axes[2].legend()\n",
    "axes[2].grid(True)\n",
    "\n",
    "# Hide the unused subplot\n",
    "axes[3].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"hebbian_models_comparison_new.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Code \n",
    "#### From: https://github.com/nosratullah/hopfieldNeuralNetwork?tab=readme-ov-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc as sp\n",
    "import matplotlib.image as img\n",
    "\n",
    "# import the image and extract\n",
    "def imageGenerator(imageVector):\n",
    "    cleanImage = np.zeros([len(imageVector)-1,len(imageVector)-1])\n",
    "    for i in range(len(imageVector)-1):\n",
    "        for j in range(len(imageVector)-1):\n",
    "            if (imageVector[i][j] > 1):\n",
    "                cleanImage[i][j] = 1\n",
    "            else:\n",
    "                cleanImage[i][j] = -1\n",
    "    noisyImage = cleanImage + np.random.normal(0, 2, [len(image)-1,len(image)-1])\n",
    "\n",
    "    for i in range(len(image)-1):\n",
    "        for j in range(len(image)-1):\n",
    "            if (noisyImage[i][j] >= 0):\n",
    "                noisyImage[i][j] = 1\n",
    "            else:\n",
    "                noisyImage[i][j] = -1\n",
    "\n",
    "    return cleanImage,noisyImage\n",
    "# Building up the coefficient matrix\n",
    "def trainer(vector,oldCoefMat):\n",
    "    vector = vector.flatten()\n",
    "    coefMat = np.zeros([len(vector)-1,len(vector)-1])\n",
    "    if (np.isscalar(oldCoefMat)):\n",
    "        for i in range(len(vector)-1):\n",
    "            for j in range(len(vector)-1):\n",
    "                if (i!=(i-j)):\n",
    "                    coefMat[i][i-j] = vector[i]*vector[i-j]\n",
    "    if (np.shape(oldCoefMat) == np.shape(coefMat)):\n",
    "        for i in range(len(vector)-1):\n",
    "            for j in range(len(vector)-1):\n",
    "                if (i!=(i-j)):\n",
    "                    coefMat[i][i-j] = vector[i]*vector[i-j]\n",
    "        coefMat = coefMat + oldCoefMat\n",
    "\n",
    "    vector = np.reshape(vector, [int(np.sqrt(len(vector))),int(np.sqrt(len(vector)))])\n",
    "    return coefMat\n",
    "\n",
    "#\n",
    "def prediction(curuptedVec,coefMat):\n",
    "    curuptedVec = curuptedVec.flatten()\n",
    "    predictVec = np.zeros(len(curuptedVec))\n",
    "    for i in range(len(curuptedVec)-1):\n",
    "        temp = 0\n",
    "        for j in range(len(curuptedVec)-1):\n",
    "             temp += coefMat[i][j] * curuptedVec[j]\n",
    "        if (temp>0):\n",
    "            predictVec[i] = 1\n",
    "        if (temp<0):\n",
    "            predictVec[i] = -1\n",
    "\n",
    "    predictVec = np.reshape(predictVec, [int(np.sqrt(len(predictVec))),int(np.sqrt(len(predictVec)))])\n",
    "    return predictVec\n",
    "\n",
    "\n",
    "#Import the images\n",
    "plt.figure(figsize=(15,10))\n",
    "for i in range(1,4):\n",
    "    image = img.imread('dataset/pgms/{}.png'.format(i),'w').copy()\n",
    "    if (i==1):\n",
    "        vector,noisyVec = imageGenerator(image)\n",
    "        coefMatrix = trainer(vector,0)\n",
    "        predictedVec = prediction(noisyVec,coefMatrix)\n",
    "    else:\n",
    "        vector,noisyVec = imageGenerator(image)\n",
    "        coefMatrix = trainer(vector,coefMatrix)\n",
    "        predictedVec = prediction(noisyVec,coefMatrix)\n",
    "\n",
    "    plt.subplot(i,4,1)\n",
    "    plt.imshow(image)\n",
    "    plt.title('Imported Picture 1')\n",
    "    plt.subplot(i,4,2)\n",
    "    plt.imshow(vector);\n",
    "    plt.title('Cleaned and Squared Picture 1')\n",
    "    plt.subplot(i,4,3)\n",
    "    plt.imshow(noisyVec);\n",
    "    plt.title('Noisy Picture 1')\n",
    "    plt.subplot(i,4,4)\n",
    "    plt.imshow(predictedVec);\n",
    "    plt.title('Recalled Picture 1')\n",
    "\n",
    "plt.savefig('hopfields.png')\n",
    "plt.clf()\n",
    "plt.imshow(coefMatrix)\n",
    "plt.savefig('matrix.png')\n",
    "plt.title('Coefficient Matrix')\n",
    "plt.show()\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
