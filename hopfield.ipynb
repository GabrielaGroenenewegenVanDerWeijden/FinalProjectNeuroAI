{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final NeuroAI Project\n",
    "## Group 1: Femke, Tikva and Gabriela"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuroAI Final Project: Hopfield network pattern retrieval\n",
    "## Group 1: Femka, Tikva & Gabriela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing everything important. \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc as sp\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seting the standard seed for reproducable results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10)  # any number, just for consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_balanced_images(folder, image_size=(32, 32), n_per_class=None):\n",
    "    \"\"\"Load the same number of grayscale images per class (e.g. cats/dogs).\"\"\"\n",
    "    classes = sorted(os.listdir(folder))  # ['cats', 'dogs']\n",
    "    all_images, all_labels = [], []\n",
    "    \n",
    "    for label in classes:\n",
    "        class_folder = os.path.join(folder, label)\n",
    "        paths = []\n",
    "        for ext in ('*.png', '*.jpg', '*.jpeg'):\n",
    "            paths.extend(glob(os.path.join(class_folder, ext)))\n",
    "        random.shuffle(paths)\n",
    "        \n",
    "        if n_per_class:\n",
    "            paths = paths[:n_per_class]  # limit per class\n",
    "        \n",
    "        for path in paths:\n",
    "            img = Image.open(path).convert('L').resize(image_size)\n",
    "            img = np.array(img, dtype=np.float32) / 255.0\n",
    "            all_images.append(img)\n",
    "            all_labels.append(label)\n",
    "    \n",
    "    # Mix the dataset\n",
    "    combined = list(zip(all_images, all_labels))\n",
    "    random.shuffle(combined)\n",
    "    all_images, all_labels = zip(*combined)\n",
    "    \n",
    "    print(f\"Loaded {len(all_images)} images from {folder} \"\n",
    "          f\"({len(classes)} classes, {n_per_class} per class)\")\n",
    "    \n",
    "    return list(all_images), list(all_labels)\n",
    "\n",
    "\n",
    "\n",
    "def imageGenerator(imageVector, binary=None):\n",
    "    imageVector = imageVector.astype(float)\n",
    "    imageVector /= np.max(imageVector)\n",
    "    # If binary is set to True, then the image gets binarised. Otherwise (else) it is not, thus staying in grayscale.\n",
    "    if binary: \n",
    "        cleanImage = np.where(imageVector > 0.5, 1, -1)\n",
    "        noisyImage = cleanImage + np.random.normal(0, 0.8, cleanImage.shape)\n",
    "        noisyImage = np.where(noisyImage >= 0, 1, -1)\n",
    "    else: \n",
    "        # Continuous case\n",
    "        cleanImage = 2 * (imageVector / np.max(imageVector)) - 1  # in [-1, 1]\n",
    "        noisyImage = cleanImage + np.random.normal(0, 0.15, cleanImage.shape) # Making it less noisier when grayscale, otherwise totally random picture with white/black dots.\n",
    "        noisyImage = np.clip(noisyImage, -1, 1)        # keep within range, no binarization\n",
    "    return cleanImage, noisyImage\n",
    "\n",
    "\n",
    "def hebbian_trainer(pattern, old_weights=None):\n",
    "    \"\"\"\n",
    "    Simple Hebbian learning rule for Hopfield network.\n",
    "    \n",
    "    pattern : 1D numpy array of Â±1 values\n",
    "    weights : existing weight matrix or None (for first pattern)\n",
    "    \"\"\"\n",
    "    # Flatten pattern in case it's an image\n",
    "    pattern = pattern.flatten().astype(np.float32)\n",
    "    N = len(pattern)\n",
    "\n",
    "    # Compute outer product: core Hebbian learning\n",
    "    # new_weights = np.outer(pattern, pattern)\n",
    "\n",
    "    # Inititalizing new_weights with all zeros.\n",
    "    new_weights = np.zeros((N, N))\n",
    "\n",
    "\n",
    "    # --- Hebbian learning (manual outer product) ---\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if i != j:  # no self-connection\n",
    "                new_weights[i, j] = pattern[i] * pattern[j]\n",
    "\n",
    "    # No self-connections\n",
    "    np.fill_diagonal(new_weights, 0)\n",
    "\n",
    "    # Normalize by vector length to avoid huge activations, to make it appear grayscale.\n",
    "    new_weights /= N\n",
    "\n",
    "    # If no old weights, start fresh; otherwise, add to them\n",
    "    if old_weights is None:\n",
    "        return new_weights\n",
    "    else:\n",
    "        return old_weights + new_weights\n",
    "    \n",
    "     # --- Storkey learning (type of hebbian learning to improve memory interference)\n",
    "     \n",
    "def storkey_rule(pattern, old_weights=None):\n",
    "    pattern = pattern.flatten().astype(np.float32)\n",
    "    N = len(pattern)\n",
    "    \n",
    "    if old_weights is None:\n",
    "        old_weights = np.zeros((N, N), dtype=np.float32)\n",
    "        \n",
    "    # --- Compute local field (net input) ---\n",
    "    h = np.dot(old_weights, pattern) \n",
    "\n",
    "    # --- Compute Hebbian and correction terms ---\n",
    "    hebbian_term = np.outer(pattern, pattern) - np.identity(N)\n",
    "\n",
    "\n",
    "    # Inititlizing the pre-synaptic weights matrix\n",
    "    pre_synaptic = np.zeros((N, len(h)))\n",
    "\n",
    "    for i in range(N):\n",
    "        for j in range(len(h)):\n",
    "            if i != j:  # no self-connection\n",
    "                pre_synaptic[i, j] = pattern[i] * h[j]\n",
    "\n",
    "    # initializing the post-synaptic weights matrix\n",
    "    post_synaptic = np.zeros((len(h), N))\n",
    "    for i in range(len(h)):\n",
    "        for j in range(N):\n",
    "            post_synaptic[i,j] = h[i] * pattern[j]\n",
    "\n",
    "    # --- Update rule ---\n",
    "    delta_w = (hebbian_term - pre_synaptic - post_synaptic) / N\n",
    "    new_weights = old_weights + delta_w\n",
    "\n",
    "    # No self-connections\n",
    "    np.fill_diagonal(new_weights, 0)\n",
    "\n",
    "    return new_weights\n",
    "\n",
    "def prediction(corruptedVec, coefMat, binary):\n",
    "    \"\"\"Hopfield recall: one synchronous update.\"\"\"\n",
    "    corruptedVec = corruptedVec.flatten()\n",
    "    \n",
    "    # If binary, we want to recall a binary pattern. Otherwise grayscale difference in sign and tanh function.\n",
    "    if binary:\n",
    "        predictVec = np.sign(np.dot(coefMat, corruptedVec))\n",
    "        predictVec[predictVec == 0] = 1  # handle zeros\n",
    "    else: \n",
    "        predictVec = np.tanh(np.dot(coefMat, corruptedVec))\n",
    "\n",
    "    side = int(np.sqrt(len(predictVec)))\n",
    "    return predictVec.reshape((side, side))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_reconstruction(selected_images, selected_labels, coefMatrix, imageGenerator, prediction, save_path=None, binary=None):\n",
    "    \"\"\"\n",
    "    Plot Original, Cleaned, Noisy, and Recalled images for each selected image.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    selected_images : list of np.ndarray\n",
    "        List of images (grayscale, as numpy arrays).\n",
    "    selected_labels : list of str\n",
    "        Corresponding labels for the images (e.g., 'dog', 'cat').\n",
    "    coefMatrix : np.ndarray\n",
    "        Hopfield network weight matrix after training.\n",
    "    imageGenerator : function\n",
    "        Function that generates (cleanImage, noisyImage) from an input image.\n",
    "    prediction : function\n",
    "        Function that recalls an image given a noisy input and the weight matrix.\n",
    "    save_path : str or None\n",
    "        Optional file path to save the resulting figure. If None, figure is only shown.\n",
    "    \"\"\"\n",
    "    n_images = len(selected_images)\n",
    "    plt.figure(figsize=(15, 5 * n_images))\n",
    "\n",
    "    for i, img in enumerate(selected_images):\n",
    "        clean, noisyVec = imageGenerator(img, binary)\n",
    "        predictedVec = prediction(noisyVec, coefMatrix, binary)\n",
    "\n",
    "        # --- Original ---\n",
    "        plt.subplot(n_images, 4, 4 * i + 1)\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.title(f'Original ({selected_labels[i]})')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # --- Cleaned ---\n",
    "        plt.subplot(n_images, 4, 4 * i + 2)\n",
    "        plt.imshow(clean, cmap='gray')\n",
    "        plt.title('Cleaned')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # --- Noisy ---\n",
    "        plt.subplot(n_images, 4, 4 * i + 3)\n",
    "        plt.imshow(noisyVec, cmap='gray')\n",
    "        plt.title('Noisy')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # --- Recalled ---\n",
    "        plt.subplot(n_images, 4, 4 * i + 4)\n",
    "        if binary: \n",
    "            plt.imshow(predictedVec, cmap='gray')\n",
    "        else: \n",
    "            plt.imshow((predictedVec + 1) / 2, cmap='gray')\n",
    "        plt.title('Recalled')\n",
    "        plt.axis('off')\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # --- Save before showing --- Commented this out, so there are no new pictures made every single time. \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hopfield_experiment(train_dir=\"data/train\", test_dir=\"data/test\", train_counts=None, \n",
    "                            learning_rule=\"hebbian\", hebbian_trainer=None, storkey_rule=None,\n",
    "                            imageGenerator=None, prediction=None, plot_image_reconstruction=None, image_size=(32, 32), binary=None):\n",
    "   \n",
    "    if train_counts is None:\n",
    "        train_counts = list(range(1, 13))\n",
    "\n",
    "    # --- Load images ---\n",
    "    train_images, train_labels = load_balanced_images(\"data/train\", image_size=image_size, n_per_class=75)\n",
    "    test_images, test_labels   = load_balanced_images(\"data/test\",  image_size=image_size, n_per_class=24)\n",
    "\n",
    "\n",
    "    # Checking the length of the images. \n",
    "    print(\"Len of test images: \", len(test_images[0].flatten()))\n",
    "\n",
    "   \n",
    "    results = []\n",
    "    conf_matrices = []\n",
    "    train_labels_list = []\n",
    "\n",
    "    for n_train in train_counts:\n",
    "        print(f\"\\n=== Training with {n_train} patterns ===\")\n",
    "        selected_indices = random.sample(range(len(train_images)), n_train)\n",
    "        selected_images = [train_images[i] for i in selected_indices]\n",
    "        selected_labels = [train_labels[i] for i in selected_indices]\n",
    "\n",
    "        coefMatrix = 0\n",
    "        stored_patterns = []\n",
    "\n",
    "        # --- Learning phase ---\n",
    "        for i, img in enumerate(selected_images):\n",
    "            clean, noisyVec = imageGenerator(img, binary)\n",
    "\n",
    "            if learning_rule.lower() == \"hebbian\":\n",
    "                coefMatrix = hebbian_trainer(clean, 0 if i == 0 else coefMatrix)\n",
    "            elif learning_rule.lower() == \"storkey\":\n",
    "                coefMatrix = storkey_rule(clean, 0 if i == 0 else coefMatrix)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid learning_rule. Choose 'hebbian' or 'storkey'.\")\n",
    "\n",
    "            stored_patterns.append(clean)\n",
    "\n",
    "        # --- Visualization ---\n",
    "        plot_image_reconstruction(\n",
    "            selected_images=selected_images,\n",
    "            selected_labels=selected_labels,\n",
    "            coefMatrix=coefMatrix,\n",
    "            imageGenerator=imageGenerator,\n",
    "            prediction=prediction,\n",
    "            save_path=f\"hopfield_stage_{learning_rule}_{n_train}_binary{binary}_neurons{image_size}.png\",\n",
    "            binary = binary\n",
    "        )\n",
    "\n",
    "        # --- Recall / classification test phase ---\n",
    "        y_true, y_pred = [], []\n",
    "        for test_img, true_label in zip(test_images, test_labels):\n",
    "            clean, noisy = imageGenerator(test_img, binary)\n",
    "            recalled = prediction(noisy, coefMatrix, binary)\n",
    "\n",
    "            # Compare recalled pattern with all stored patterns\n",
    "            similarities = [np.mean(recalled == p) for p in stored_patterns]\n",
    "            best_match_idx = np.argmax(similarities)\n",
    "            predicted_label = selected_labels[best_match_idx]\n",
    "\n",
    "            y_true.append(true_label)\n",
    "            y_pred.append(predicted_label)\n",
    "\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=['dogs', 'cats'])\n",
    "        conf_matrices.append(cm)\n",
    "        train_labels_list.append(n_train)\n",
    "\n",
    "        # --- Accuracy ---\n",
    "        accuracy = np.mean(np.array(y_true) == np.array(y_pred))\n",
    "        results.append(accuracy)\n",
    "        print(f\"Classification accuracy with {n_train} training images (binary: {binary}, rule: {learning_rule},neuons:{image_size}): {accuracy:.2f}\")\n",
    "\n",
    "    return results, conf_matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_counts = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Base model\n",
    "results_hebbian, confs_hebbian = run_hopfield_experiment(\n",
    "    learning_rule=\"hebbian\",\n",
    "    hebbian_trainer=hebbian_trainer,\n",
    "    train_counts= train_counts,\n",
    "    storkey_rule=storkey_rule,\n",
    "    imageGenerator=imageGenerator,\n",
    "    prediction=prediction,\n",
    "    plot_image_reconstruction=plot_image_reconstruction,\n",
    "    image_size=(32, 32),\n",
    "    binary = True\n",
    ")\n",
    "\n",
    "np.savez(\"results_hebbian.npz\", results=results_hebbian, confs=confs_hebbian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base model with non-binary\n",
    "results_non_binary, confs_non_binary = run_hopfield_experiment(\n",
    "    learning_rule=\"hebbian\",\n",
    "    hebbian_trainer=hebbian_trainer,\n",
    "    train_counts= train_counts,\n",
    "    storkey_rule=storkey_rule,\n",
    "    imageGenerator=imageGenerator,\n",
    "    prediction=prediction,\n",
    "    plot_image_reconstruction=plot_image_reconstruction,\n",
    "    image_size=(32, 32),\n",
    "    binary= False # Continuous values\n",
    ")\n",
    "\n",
    "np.savez(\"results_non_binary.npz\", results=results_non_binary, confs=confs_non_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base model with more neurons\n",
    "results_more_neurons, confs_more_neurons = run_hopfield_experiment(\n",
    "    learning_rule=\"hebbian\",\n",
    "    hebbian_trainer=hebbian_trainer,\n",
    "    train_counts= train_counts,\n",
    "    storkey_rule=storkey_rule,\n",
    "    imageGenerator=imageGenerator,\n",
    "    prediction=prediction,\n",
    "    plot_image_reconstruction=plot_image_reconstruction,\n",
    "    image_size=(128, 128), # More neurons\n",
    "    binary = True \n",
    ")\n",
    "\n",
    "np.savez(\"results_more_neurons.npz\", results=results_more_neurons, confs=confs_more_neurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting learning improvement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_hebbian = \"#9b5de5\"      # violet-purple\n",
    "color_more_neurons = \"#f15bb5\" # hot pink\n",
    "color_non_binary = \"#00bbf9\"   # cyan\n",
    "\n",
    "# --- Create figure with subplots ---\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
    "axes = axes.flatten()  # make it easy to index as a 1D array\n",
    "\n",
    "# --- Plot 1: Base model ---\n",
    "axes[0].plot(train_counts, results_hebbian, 'o-', color=color_hebbian)\n",
    "axes[0].set_title(\"Base Hebbian model\")\n",
    "axes[0].set_xlabel(\"Training images\")\n",
    "axes[0].set_ylabel(\"Accuracy\")\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[0].grid(True)\n",
    "\n",
    "# --- Plot 2: More neurons model ---\n",
    "axes[1].plot(train_counts, results_more_neurons, 's-', color=color_more_neurons)\n",
    "axes[1].set_title(\"More neurons model\")\n",
    "axes[1].set_xlabel(\"Training images\")\n",
    "axes[1].set_ylabel(\"Accuracy\")\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].grid(True)\n",
    "\n",
    "# --- Plot 3: Non-binary model ---\n",
    "axes[2].plot(train_counts, results_non_binary, '^-', color=color_non_binary)\n",
    "axes[2].set_title(\"Non-binary model\")\n",
    "axes[2].set_xlabel(\"Training images\")\n",
    "axes[2].set_ylabel(\"Accuracy\")\n",
    "axes[2].set_ylim(0, 1)\n",
    "axes[2].grid(True)\n",
    "\n",
    "# --- Plot 4: Combined comparison ---\n",
    "axes[3].plot(train_counts, results_hebbian, 'o-', color=color_hebbian, label='Base model')\n",
    "axes[3].plot(train_counts, results_more_neurons, 's-', color=color_more_neurons, label='More neurons')\n",
    "axes[3].plot(train_counts, results_non_binary, '^-',  color=color_non_binary, label='Non-binary')\n",
    "axes[3].set_title(\"All models compared\")\n",
    "axes[3].set_xlabel(\"Training images\")\n",
    "axes[3].set_ylabel(\"Accuracy\")\n",
    "axes[3].set_ylim(0, 1)\n",
    "axes[3].legend()\n",
    "axes[3].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting all the confusion matrices together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Base Hebbian\": {\n",
    "        \"results\": results_hebbian,\n",
    "        \"confs\": confs_hebbian,\n",
    "        \"color\": \"Purples\"\n",
    "    },\n",
    "    \"More neurons\": {\n",
    "        \"results\": results_more_neurons,\n",
    "        \"confs\": confs_more_neurons,\n",
    "        \"color\": plt.cm.RdPu\n",
    "    },\n",
    "    \"Non-binary\": {\n",
    "        \"results\": results_non_binary,\n",
    "        \"confs\": confs_non_binary,\n",
    "        \"color\": \"Blues\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Plotting the confusion matrices\n",
    "for model_name, model_data in models.items():\n",
    "    conf_matrices = model_data[\"confs\"]\n",
    "    color = model_data[\"color\"]\n",
    "\n",
    "    num_plots = len(conf_matrices)\n",
    "    cols = 3\n",
    "    rows = int(np.ceil(num_plots / cols))\n",
    "\n",
    "    plt.figure(figsize=(5 * cols, 5 * rows))\n",
    "\n",
    "    for i, (cm, n_train) in enumerate(zip(conf_matrices, train_counts)):\n",
    "        ax = plt.subplot(rows, cols, i + 1)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['dog', 'cat'])\n",
    "        disp.plot(cmap=color, values_format='d', ax=ax, colorbar=False)\n",
    "        ax.set_title(f\"{n_train} training images\")\n",
    "\n",
    "    plt.suptitle(f\"{model_name}: Confusion Matrices Across Training Levels\", fontsize=18)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Code \n",
    "#### From: https://github.com/nosratullah/hopfieldNeuralNetwork?tab=readme-ov-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc as sp\n",
    "import matplotlib.image as img\n",
    "\n",
    "# import the image and extract\n",
    "def imageGenerator(imageVector):\n",
    "    cleanImage = np.zeros([len(imageVector)-1,len(imageVector)-1])\n",
    "    for i in range(len(imageVector)-1):\n",
    "        for j in range(len(imageVector)-1):\n",
    "            if (imageVector[i][j] > 1):\n",
    "                cleanImage[i][j] = 1\n",
    "            else:\n",
    "                cleanImage[i][j] = -1\n",
    "    noisyImage = cleanImage + np.random.normal(0, 2, [len(image)-1,len(image)-1])\n",
    "\n",
    "    for i in range(len(image)-1):\n",
    "        for j in range(len(image)-1):\n",
    "            if (noisyImage[i][j] >= 0):\n",
    "                noisyImage[i][j] = 1\n",
    "            else:\n",
    "                noisyImage[i][j] = -1\n",
    "\n",
    "    return cleanImage,noisyImage\n",
    "# Building up the coefficient matrix\n",
    "def trainer(vector,oldCoefMat):\n",
    "    vector = vector.flatten()\n",
    "    coefMat = np.zeros([len(vector)-1,len(vector)-1])\n",
    "    if (np.isscalar(oldCoefMat)):\n",
    "        for i in range(len(vector)-1):\n",
    "            for j in range(len(vector)-1):\n",
    "                if (i!=(i-j)):\n",
    "                    coefMat[i][i-j] = vector[i]*vector[i-j]\n",
    "    if (np.shape(oldCoefMat) == np.shape(coefMat)):\n",
    "        for i in range(len(vector)-1):\n",
    "            for j in range(len(vector)-1):\n",
    "                if (i!=(i-j)):\n",
    "                    coefMat[i][i-j] = vector[i]*vector[i-j]\n",
    "        coefMat = coefMat + oldCoefMat\n",
    "\n",
    "    vector = np.reshape(vector, [int(np.sqrt(len(vector))),int(np.sqrt(len(vector)))])\n",
    "    return coefMat\n",
    "\n",
    "#\n",
    "def prediction(curuptedVec,coefMat):\n",
    "    curuptedVec = curuptedVec.flatten()\n",
    "    predictVec = np.zeros(len(curuptedVec))\n",
    "    for i in range(len(curuptedVec)-1):\n",
    "        temp = 0\n",
    "        for j in range(len(curuptedVec)-1):\n",
    "             temp += coefMat[i][j] * curuptedVec[j]\n",
    "        if (temp>0):\n",
    "            predictVec[i] = 1\n",
    "        if (temp<0):\n",
    "            predictVec[i] = -1\n",
    "\n",
    "    predictVec = np.reshape(predictVec, [int(np.sqrt(len(predictVec))),int(np.sqrt(len(predictVec)))])\n",
    "    return predictVec\n",
    "\n",
    "\n",
    "#Import the images\n",
    "plt.figure(figsize=(15,10))\n",
    "for i in range(1,4):\n",
    "    image = img.imread('dataset/pgms/{}.png'.format(i),'w').copy()\n",
    "    if (i==1):\n",
    "        vector,noisyVec = imageGenerator(image)\n",
    "        coefMatrix = trainer(vector,0)\n",
    "        predictedVec = prediction(noisyVec,coefMatrix)\n",
    "    else:\n",
    "        vector,noisyVec = imageGenerator(image)\n",
    "        coefMatrix = trainer(vector,coefMatrix)\n",
    "        predictedVec = prediction(noisyVec,coefMatrix)\n",
    "\n",
    "    plt.subplot(i,4,1)\n",
    "    plt.imshow(image)\n",
    "    plt.title('Imported Picture 1')\n",
    "    plt.subplot(i,4,2)\n",
    "    plt.imshow(vector);\n",
    "    plt.title('Cleaned and Squared Picture 1')\n",
    "    plt.subplot(i,4,3)\n",
    "    plt.imshow(noisyVec);\n",
    "    plt.title('Noisy Picture 1')\n",
    "    plt.subplot(i,4,4)\n",
    "    plt.imshow(predictedVec);\n",
    "    plt.title('Recalled Picture 1')\n",
    "\n",
    "plt.savefig('hopfields.png')\n",
    "plt.clf()\n",
    "plt.imshow(coefMatrix)\n",
    "plt.savefig('matrix.png')\n",
    "plt.title('Coefficient Matrix')\n",
    "plt.show()\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
